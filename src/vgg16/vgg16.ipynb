{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os, sys, getopt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "from pathlib import Path\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import handshape_datasets as hd\n",
    "\n",
    "def load(dataset_name):\n",
    "    \"\"\"\n",
    "    Load ciarp dataset.\n",
    "\n",
    "    Returns (x, y): as dataset x and y.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset_path = os.path.join('develop','data',dataset_name,'data')\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    data = hd.load(dataset_name, Path(dataset_path))\n",
    "\n",
    "    return data[0], data[1]['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_name = \"lsa16\", rotation_range = 10, width_shift_range = 0.10,\n",
    "          height_shift_range = 0.10, horizontal_flip = True, lr = 0.001, epochs = 10,\n",
    "          max_patience = 25, batch_size= 16, checkpoints = False, weight_classes = False,\n",
    "          train_size=None, test_size=None):\n",
    "\n",
    "    # log\n",
    "    log_freq = 1\n",
    "    save_freq = 40\n",
    "    models_directory = 'models/'\n",
    "    results_directory = 'results/'\n",
    "    config_directory = 'config/'\n",
    "\n",
    "    general_directory = Path.cwd() / \"results/\"\n",
    "    save_directory = general_directory / \"{}/transfer-learning/\".format(dataset_name)\n",
    "    results = 'epoch,loss,accuracy,test_loss,test_accuracy\\n'\n",
    "\n",
    "    date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "    identifier = \"vgg16-{}\".format(\n",
    "        dataset_name) + date\n",
    "\n",
    "    summary_file = general_directory / 'summary.csv'\n",
    "\n",
    "    # create summary file if not exists\n",
    "    if not summary_file.exists():\n",
    "        file = open(summary_file, 'w')\n",
    "        file.write(\"datetime, model, config, min_loss, min_loss_accuracy\\n\")\n",
    "        file.close()\n",
    "\n",
    "    print(\"hyperparameters set\")\n",
    "    #print(tf.test.is_gpu_available())\n",
    "\n",
    "    x, y = load(dataset_name)\n",
    "\n",
    "    image_shape = np.shape(x)[1:]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                        y,\n",
    "                                                        train_size=train_size,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y)\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    n_classes = len(np.unique(y))\n",
    "\n",
    "    if weight_classes:\n",
    "        class_weights = compute_class_weight('balanced', \n",
    "                                            np.unique(y),\n",
    "                                            y)\n",
    "    \n",
    "    print(\"data loaded\")\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=rotation_range,\n",
    "        width_shift_range=width_shift_range,\n",
    "        height_shift_range=height_shift_range,\n",
    "        horizontal_flip=horizontal_flip,\n",
    "        fill_mode='constant',\n",
    "        cval=0)\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        fill_mode='constant',\n",
    "        cval=0)\n",
    "    test_datagen.fit(x_train)\n",
    "\n",
    "    # get the model\n",
    "    base_model = VGG16(include_top=False, weights='none', input_shape=image_shape)\n",
    "\n",
    "    global_average_layer = GlobalAveragePooling2D()\n",
    "    hidden_dense_layer = Dense(1024, activation='relu')\n",
    "    prediction_layer = Dense(n_classes, activation='softmax')\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        global_average_layer,\n",
    "        hidden_dense_layer,\n",
    "        prediction_layer\n",
    "    ])\n",
    "    \n",
    "    print(\"model created\")\n",
    "\n",
    "    if weight_classes:\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        def weightedLoss(originalLossFunc, weightsList):\n",
    "\n",
    "            @tf.function\n",
    "            def lossFunc(true, pred):\n",
    "\n",
    "                axis = -1 #if channels last \n",
    "                #axis=  1 #if channels first\n",
    "\n",
    "                #argmax returns the index of the element with the greatest value\n",
    "                #done in the class axis, it returns the class index    \n",
    "                classSelectors = tf.argmax(true, axis=axis, output_type=tf.int32) \n",
    "\n",
    "                #considering weights are ordered by class, for each class\n",
    "                #true(1) if the class index is equal to the weight index   \n",
    "                classSelectors = [tf.equal(i, classSelectors) for i in range(len(weightsList))]\n",
    "\n",
    "                #casting boolean to float for calculations  \n",
    "                #each tensor in the list contains 1 where ground true class is equal to its index \n",
    "                #if you sum all these, you will get a tensor full of ones. \n",
    "                classSelectors = [tf.cast(x, tf.float32) for x in classSelectors]\n",
    "\n",
    "                #for each of the selections above, multiply their respective weight\n",
    "                weights = [sel * w for sel,w in zip(classSelectors, weightsList)] \n",
    "\n",
    "                #sums all the selections\n",
    "                #result is a tensor with the respective weight for each element in predictions\n",
    "                weightMultiplier = weights[0]\n",
    "                for i in range(1, len(weights)):\n",
    "                    weightMultiplier = weightMultiplier + weights[i]\n",
    "\n",
    "\n",
    "                #make sure your originalLossFunc only collapses the class axis\n",
    "                #you need the other axes intact to multiply the weights tensor\n",
    "                loss = originalLossFunc(true,pred) \n",
    "                loss = loss * weightMultiplier\n",
    "\n",
    "                return loss\n",
    "            return lossFunc\n",
    "        loss_object = weightedLoss(loss_object, class_weights)\n",
    "    else:\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer=optimizer, loss=loss_object)\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "            loss = loss_object(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, predictions)\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(images, labels):\n",
    "        predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "        t_loss = loss_object(labels, predictions)\n",
    "\n",
    "        test_loss(t_loss)\n",
    "        test_accuracy(labels, predictions)\n",
    "\n",
    "    # create summary writers\n",
    "    train_summary_writer = tf.summary.create_file_writer(save_directory / ('summaries/train/' + identifier))\n",
    "    test_summary_writer = tf.summary.create_file_writer(save_directory /  ('summaries/test/' + identifier))\n",
    "\n",
    "    # create data generators\n",
    "    train_gen =  datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "    test_gen = test_datagen.flow(x_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"starting training\")\n",
    "\n",
    "    min_loss = 100\n",
    "    min_loss_acc = 0\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batches = 0\n",
    "        for images, labels in train_gen:\n",
    "            train_step(images, labels)\n",
    "            batches += 1\n",
    "            if batches >= len(x_train) / 32:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        batches = 0\n",
    "        for test_images, test_labels in test_gen:\n",
    "            test_step(test_images, test_labels)\n",
    "            batches += 1\n",
    "            if batches >= len(x_test) / 32:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        if (epoch % log_freq == 0):\n",
    "            results += '{},{},{},{},{}\\n'.format(epoch,\n",
    "                                train_loss.result(),\n",
    "                                train_accuracy.result()*100,\n",
    "                                test_loss.result(),\n",
    "                                test_accuracy.result()*100)\n",
    "            print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}'.format(epoch,\n",
    "                                train_loss.result(),\n",
    "                                train_accuracy.result()*100,\n",
    "                                test_loss.result(),\n",
    "                                test_accuracy.result()*100))\n",
    "\n",
    "            if (test_loss.result() < min_loss):    \n",
    "                if not (save_directory / models_directory).exists():\n",
    "                    os.makedirs(save_directory / models_directory)\n",
    "                # serialize weights to HDF5\n",
    "                model.save_weights(save_directory / (models_directory + \"best{}.h5\".format(identifier)))\n",
    "                min_loss = test_loss.result()\n",
    "                min_loss_acc = test_accuracy.result()\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "                tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "                train_loss.reset_states()           \n",
    "                train_accuracy.reset_states()           \n",
    "\n",
    "            with test_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "                tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "                test_loss.reset_states()           \n",
    "                test_accuracy.reset_states()   \n",
    "                \n",
    "        if checkpoints and epoch % save_freq == 0:\n",
    "            if not (save_directory / models_directory).exists():\n",
    "                os.makedirs(save_directory / models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(save_directory / (models_directory+\"{}_epoch{}.h5\".format(identifier,epoch)))\n",
    "            \n",
    "        if patience >= max_patience:\n",
    "            break\n",
    "\n",
    "    if not (save_directory / results_directory).exists():\n",
    "        os.makedirs(save_directory / results_directory)\n",
    "    file = open(save_directory / (results_directory + 'results-'+ identifier + '.csv'),'w') \n",
    "    file.write(results) \n",
    "    file.close()\n",
    "\n",
    "    if not (save_directory / config_directory).exists():\n",
    "        os.makedirs(save_directory / config_directory)\n",
    "\n",
    "    config = {\n",
    "        'data.dataset_name': dataset_name, \n",
    "        'data.rotation_range': rotation_range, \n",
    "        'data.width_shift_range': width_shift_range, \n",
    "        'data.height_shift_range': height_shift_range, \n",
    "        'data.horizontal_flip': horizontal_flip, \n",
    "        'model.reduction': reduction, \n",
    "        'train.lr': lr, \n",
    "        'train.epochs': epochs, \n",
    "        'train.max_patience': max_patience, \n",
    "        'train.batch_size': batch_size, \n",
    "    }\n",
    "\n",
    "    file = open(save_directory / (config_directory + identifier + '.json'), 'w')\n",
    "    file.write(json.dumps(config, indent=2))\n",
    "    file.close()\n",
    "\n",
    "    file = open(summary_file, 'a+')\n",
    "    summary = \"{}, {}, vgg, {}, {}, {}\\n\".format(date,\n",
    "                                                       dataset_name,\n",
    "                                                       save_directory / (config_directory + identifier + '.json'),\n",
    "                                                       min_loss,\n",
    "                                                       min_loss_acc)\n",
    "    file.write(summary)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InceptionV3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-c87d3b6a8963>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create the base pre-trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbase_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# add a global spatial average pooling layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InceptionV3' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(...)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
