{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os, sys, getopt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import Model\n",
    "from pathlib import Path\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import handshape_datasets as hd\n",
    "\n",
    "def load(dataset_name):\n",
    "    \"\"\"\n",
    "    Load ciarp dataset.\n",
    "\n",
    "    Returns (x, y): as dataset x and y.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset_path = os.path.join('develop','data',dataset_name,'data')\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    data = hd.load(dataset_name, Path(dataset_path))\n",
    "\n",
    "    return data[0], data[1]['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_name = \"lsa16\", rotation_range = 10, width_shift_range = 0.10,\n",
    "          height_shift_range = 0.10, horizontal_flip = True,\n",
    "          model_type='DenseNet121', lr = 0.001, epochs = 10,\n",
    "          max_patience = 25, batch_size= 16, checkpoints = False, weight_classes = False,\n",
    "          train_size=None, test_size=None):\n",
    "\n",
    "    # log\n",
    "    log_freq = 1\n",
    "    save_freq = 40\n",
    "    models_directory = 'models/'\n",
    "    results_directory = 'results/'\n",
    "    config_directory = 'config/'\n",
    "\n",
    "    general_directory = Path.cwd() / \"results/\"\n",
    "    save_directory = general_directory / \"{}/transfer-learning/\".format(dataset_name)\n",
    "    results = 'epoch,loss,accuracy,test_loss,test_accuracy\\n'\n",
    "\n",
    "    date = datetime.now().strftime(\"%Y_%m_%d-%H:%M:%S\")\n",
    "    identifier = \"densenet_transfer_learning-{}\".format(\n",
    "        dataset_name) + date\n",
    "\n",
    "    summary_file = general_directory / 'summary.csv'\n",
    "\n",
    "    # create summary file if not exists\n",
    "    if not summary_file.exists():\n",
    "        file = open(summary_file, 'w')\n",
    "        file.write(\"datetime, model, config, min_loss, min_loss_accuracy\\n\")\n",
    "        file.close()\n",
    "\n",
    "    print(\"hyperparameters set\")\n",
    "    #print(tf.test.is_gpu_available())\n",
    "\n",
    "    x, y = load(dataset_name)\n",
    "\n",
    "    image_shape = np.shape(x)[1:]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x,\n",
    "                                                        y,\n",
    "                                                        train_size=train_size,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y)\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    n_classes = len(np.unique(y))\n",
    "\n",
    "    if weight_classes:\n",
    "        class_weights = compute_class_weight('balanced', \n",
    "                                            np.unique(y),\n",
    "                                            y)\n",
    "    \n",
    "    print(\"data loaded\")\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=rotation_range,\n",
    "        width_shift_range=width_shift_range,\n",
    "        height_shift_range=height_shift_range,\n",
    "        horizontal_flip=horizontal_flip,\n",
    "        fill_mode='constant',\n",
    "        cval=0)\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        fill_mode='constant',\n",
    "        cval=0)\n",
    "    test_datagen.fit(x_train)\n",
    "\n",
    "    # get the model\n",
    "    if (model_type == 'DenseNet121'):\n",
    "        base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=image_shape)\n",
    "    elif (model_type == 'DenseNet169'):\n",
    "        base_model = DenseNet169(include_top=False, weights='imagenet', input_shape=image_shape)\n",
    "    elif (model_type == 'DenseNet201'):\n",
    "        base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=image_shape)\n",
    "    else: \n",
    "        print('Error: Wrong model type (DenseNet121,DenseNet169,DenseNet201)')\n",
    "        return\n",
    "\n",
    "    base_model.trainable = False\n",
    "    global_average_layer = GlobalAveragePooling2D()\n",
    "    hidden_dense_layer = Dense(1024, activation='relu')\n",
    "    prediction_layer = Dense(n_classes, activation='softmax')\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        global_average_layer,\n",
    "        hidden_dense_layer,\n",
    "        prediction_layer\n",
    "    ])\n",
    "    \n",
    "    print(\"model created\")\n",
    "\n",
    "    if weight_classes:\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "        def weightedLoss(originalLossFunc, weightsList):\n",
    "\n",
    "            @tf.function\n",
    "            def lossFunc(true, pred):\n",
    "\n",
    "                axis = -1 #if channels last \n",
    "                #axis=  1 #if channels first\n",
    "\n",
    "                #argmax returns the index of the element with the greatest value\n",
    "                #done in the class axis, it returns the class index    \n",
    "                classSelectors = tf.argmax(true, axis=axis, output_type=tf.int32) \n",
    "\n",
    "                #considering weights are ordered by class, for each class\n",
    "                #true(1) if the class index is equal to the weight index   \n",
    "                classSelectors = [tf.equal(i, classSelectors) for i in range(len(weightsList))]\n",
    "\n",
    "                #casting boolean to float for calculations  \n",
    "                #each tensor in the list contains 1 where ground true class is equal to its index \n",
    "                #if you sum all these, you will get a tensor full of ones. \n",
    "                classSelectors = [tf.cast(x, tf.float32) for x in classSelectors]\n",
    "\n",
    "                #for each of the selections above, multiply their respective weight\n",
    "                weights = [sel * w for sel,w in zip(classSelectors, weightsList)] \n",
    "\n",
    "                #sums all the selections\n",
    "                #result is a tensor with the respective weight for each element in predictions\n",
    "                weightMultiplier = weights[0]\n",
    "                for i in range(1, len(weights)):\n",
    "                    weightMultiplier = weightMultiplier + weights[i]\n",
    "\n",
    "\n",
    "                #make sure your originalLossFunc only collapses the class axis\n",
    "                #you need the other axes intact to multiply the weights tensor\n",
    "                loss = originalLossFunc(true,pred) \n",
    "                loss = loss * weightMultiplier\n",
    "\n",
    "                return loss\n",
    "            return lossFunc\n",
    "        loss_object = weightedLoss(loss_object, class_weights)\n",
    "    else:\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # compile the model (should be done *after* setting layers to non-trainable)\n",
    "    model.compile(optimizer=optimizer, loss=loss_object)\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(images, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(tf.cast(images, tf.float32), training=True)\n",
    "            loss = loss_object(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, predictions)\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(images, labels):\n",
    "        predictions = model(tf.cast(images, tf.float32), training=False)\n",
    "        t_loss = loss_object(labels, predictions)\n",
    "\n",
    "        test_loss(t_loss)\n",
    "        test_accuracy(labels, predictions)\n",
    "\n",
    "    # create summary writers\n",
    "    train_summary_writer = tf.summary.create_file_writer(save_directory / ('summaries/train/' + identifier))\n",
    "    test_summary_writer = tf.summary.create_file_writer(save_directory /  ('summaries/test/' + identifier))\n",
    "\n",
    "    # create data generators\n",
    "    train_gen =  datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "    test_gen = test_datagen.flow(x_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"starting training\")\n",
    "\n",
    "    min_loss = 100\n",
    "    min_loss_acc = 0\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batches = 0\n",
    "        for images, labels in train_gen:\n",
    "            train_step(images, labels)\n",
    "            batches += 1\n",
    "            if batches >= len(x_train) / 32:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        batches = 0\n",
    "        for test_images, test_labels in test_gen:\n",
    "            test_step(test_images, test_labels)\n",
    "            batches += 1\n",
    "            if batches >= len(x_test) / 32:\n",
    "                # we need to break the loop by hand because\n",
    "                # the generator loops indefinitely\n",
    "                break\n",
    "\n",
    "        if (epoch % log_freq == 0):\n",
    "            results += '{},{},{},{},{}\\n'.format(epoch,\n",
    "                                train_loss.result(),\n",
    "                                train_accuracy.result()*100,\n",
    "                                test_loss.result(),\n",
    "                                test_accuracy.result()*100)\n",
    "            print ('Epoch: {}, Train Loss: {}, Train Acc:{}, Test Loss: {}, Test Acc: {}'.format(epoch,\n",
    "                                train_loss.result(),\n",
    "                                train_accuracy.result()*100,\n",
    "                                test_loss.result(),\n",
    "                                test_accuracy.result()*100))\n",
    "\n",
    "            if (test_loss.result() < min_loss):    \n",
    "                if not (save_directory / models_directory).exists():\n",
    "                    os.makedirs(save_directory / models_directory)\n",
    "                # serialize weights to HDF5\n",
    "                model.save_weights(save_directory / (models_directory + \"best{}.h5\".format(identifier)))\n",
    "                min_loss = test_loss.result()\n",
    "                min_loss_acc = test_accuracy.result()\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "                tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "                train_loss.reset_states()           \n",
    "                train_accuracy.reset_states()           \n",
    "\n",
    "            with test_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "                tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "                test_loss.reset_states()           \n",
    "                test_accuracy.reset_states()   \n",
    "                \n",
    "        if checkpoints and epoch % save_freq == 0:\n",
    "            if not (save_directory / models_directory).exists():\n",
    "                os.makedirs(save_directory / models_directory)\n",
    "            # serialize weights to HDF5\n",
    "            model.save_weights(save_directory / (models_directory+\"{}_epoch{}.h5\".format(identifier,epoch)))\n",
    "            \n",
    "        if patience >= max_patience:\n",
    "            break\n",
    "\n",
    "    if not (save_directory / results_directory).exists():\n",
    "        os.makedirs(save_directory / results_directory)\n",
    "    file = open(save_directory / (results_directory + 'results-'+ identifier + '.csv'),'w') \n",
    "    file.write(results) \n",
    "    file.close()\n",
    "\n",
    "    if not (save_directory / config_directory).exists():\n",
    "        os.makedirs(save_directory / config_directory)\n",
    "\n",
    "    config = {\n",
    "        'data.dataset_name': dataset_name, \n",
    "        'data.rotation_range': rotation_range, \n",
    "        'data.width_shift_range': width_shift_range, \n",
    "        'data.height_shift_range': height_shift_range, \n",
    "        'data.horizontal_flip': horizontal_flip, \n",
    "        'model.reduction': reduction, \n",
    "        'train.lr': lr, \n",
    "        'train.epochs': epochs, \n",
    "        'train.max_patience': max_patience, \n",
    "        'train.batch_size': batch_size, \n",
    "    }\n",
    "\n",
    "    file = open(save_directory / (config_directory + identifier + '.json'), 'w')\n",
    "    file.write(json.dumps(config, indent=2))\n",
    "    file.close()\n",
    "\n",
    "    file = open(summary_file, 'a+')\n",
    "    summary = \"{}, {}, densenet-tranfer-learning, {}, {}, {}\\n\".format(date,\n",
    "                                                       dataset_name,\n",
    "                                                       save_directory / (config_directory + identifier + '.json'),\n",
    "                                                       min_loss,\n",
    "                                                       min_loss_acc)\n",
    "    file.write(summary)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters set\n",
      "data loaded\n",
      "model created\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (WindowsPath('C:/Users/okaso/OneDrive/Escritorio/tesis/Rios-Gaston-tesis-2020/src/transfer_learning/results/lsa16/transfer-learning/summaries/train/densenet_transfer_learning-lsa162020_02_05-21:48:03')) with an unsupported type (<class 'pathlib.WindowsPath'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-78cd8945818d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset_name, rotation_range, width_shift_range, height_shift_range, horizontal_flip, model_type, lr, epochs, max_patience, batch_size, checkpoints, weight_classes, train_size, test_size)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# create summary writers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mtrain_summary_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_directory\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'summaries/train/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[0mtest_summary_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_directory\u001b[0m \u001b[1;33m/\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[1;34m'summaries/test/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\summary_ops_v2.py\u001b[0m in \u001b[0;36mcreate_file_writer_v2\u001b[1;34m(logdir, max_queue, flush_millis, filename_suffix, name)\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mflush_millis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflush_millis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             filename_suffix=filename_suffix)\n\u001b[1;32m--> 383\u001b[1;33m       \u001b[0mlogdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmax_queue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0mmax_queue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    315\u001b[0m                                          as_ref=False):\n\u001b[0;32m    316\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    256\u001b[0m   \"\"\"\n\u001b[0;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 258\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    264\u001b[0m   \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (WindowsPath('C:/Users/okaso/OneDrive/Escritorio/tesis/Rios-Gaston-tesis-2020/src/transfer_learning/results/lsa16/transfer-learning/summaries/train/densenet_transfer_learning-lsa162020_02_05-21:48:03')) with an unsupported type (<class 'pathlib.WindowsPath'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(...)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
